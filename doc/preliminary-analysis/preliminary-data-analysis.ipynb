{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Preliminary data analysis"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Housekeepinâ€™"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# # lan model\n",
    "# import requests\n",
    "# ftm_url = \"https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.bin\"\n",
    "# r = requests.get(ftm_url, allow_redirects=True)\n",
    "# open(\"../../data/lid.176.bin\", \"wb\").write(r.content)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# docs\n",
    "docs_augmented = pd.read_pickle(\"../../data/docs-augmented.pkl\")\n",
    "docs_augmented.head(3)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "display(docs_augmented.shape[0])\n",
    "display(docs_augmented.OutputText.isnull().sum())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "docs_augmented.Year.value_counts()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "f, ax = plt.subplots(1,1,figsize=(8.5,7))\n",
    "docs_augmented.groupby(\"Country\").Participant.count().\\\n",
    "    sort_values().tail(25).plot(kind=\"barh\", ax=ax);\n",
    "ax.set_ylabel(\"country\");\n",
    "ax.set_xlabel(\"frequency\");"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "f, ax = plt.subplots(1,1,figsize=(7,7))\n",
    "docs_augmented.groupby(\"Sector\").Participant.count().\\\n",
    "    sort_values().tail(25).plot(kind=\"barh\", ax=ax);\n",
    "ax.set_ylabel(\"sector\");\n",
    "ax.set_xlabel(\"frequency\"); "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# avg documents per participant, possible concat?\n",
    "docs_augmented.Participant.value_counts().mean()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Texts"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# simple pre-proc\n",
    "def purge_chars(string):\n",
    "    import re\n",
    "    string = string.lower()\n",
    "    string = re.sub(\"<.*?>|</.*?>\",\"\", string)\n",
    "    string = re.sub(\"(s?)(f|ht)tp(s?)://\\\\S+\\\\b\",\"\",string)\n",
    "    string = re.sub(\"[^a-z@ '.,?!\\\\-:]\",\" \",string)\n",
    "    return re.sub(\"\\\\s+\",\" \", string)\n",
    "\n",
    "docs_augmented[\"OutputChars\"] = docs_augmented.apply(lambda x: purge_chars(x.OutputText), axis=1)    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# chars, no of tokens, no of sentences, languages\n",
    "def get_characteristics(df):\n",
    "    import re\n",
    "    import pandas as pd\n",
    "    import fasttext as fs\n",
    "    string = df.OutputChars\n",
    "    df[\"n_chars\"] = len(string)\n",
    "    df[\"n_words\"] = len(re.split(\"\\\\W+\",string))\n",
    "    df[\"n_sents\"] = len(re.split(\"[.!?]\", string))\n",
    "    ftm = fs.load_model(\"../../data/lid.176.bin\")\n",
    "    lest = ftm.predict(string, k=1)\n",
    "    df[\"lang_est\"] = lest[0][0].split(\"_\")[-1]\n",
    "    return df\n",
    "\n",
    "docs_augmented = docs_augmented.apply(get_characteristics, axis=1)    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "f, ax = plt.subplots(1,1,figsize=(5,5))\n",
    "docs_augmented.groupby(\"lang_est\").Participant.count().\\\n",
    "    sort_values().tail(25).plot(kind=\"barh\", ax=ax, logx=True);\n",
    "ax.set_ylabel(\"languages\");\n",
    "ax.set_xlabel(\"frequency\"); "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# plot it\n",
    "docs_augmented = docs_augmented[docs_augmented.lang_est==\"en\"]\n",
    "f, axs = plt.subplots(1,3,figsize=(15,5))\n",
    "counts = {\"n_chars\":\"no characters\", \"n_words\":\"no words\", \"n_sents\":\"no senteces\"}\n",
    "for c, ax in zip(counts.keys(), axs.flatten()):\n",
    "    docs_augmented[c].plot(kind=\"hist\", ax=ax, logy=True, rot=90);\n",
    "    ax.set_ylabel(\"frequency\");\n",
    "    ax.set_xlabel(counts[c]);"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#  the most frequent tokens\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "tokvec = CountVectorizer(max_features=25, stop_words=\"english\",\n",
    "    max_df=0.95, min_df=2)\n",
    "token_counts = tokvec.fit_transform(docs_augmented.OutputChars)\n",
    "f, ax = plt.subplots(1,1,figsize=(10,10))\n",
    "pd.DataFrame(token_counts.toarray(), columns=tokvec.get_feature_names()).\\\n",
    "    sum().sort_values().plot(kind=\"barh\", ax=ax);\n",
    "ax.set_ylabel(\"tokens\");\n",
    "ax.set_xlabel(\"freq\"); "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 1,2,3-grams\n",
    "gramvec = CountVectorizer(ngram_range=(1,3), stop_words=\"english\",\n",
    "    max_df=0.95, min_df=2)\n",
    "gram_counts = gramvec.fit_transform(docs_augmented.OutputChars)\n",
    "gram_counts = pd.DataFrame(gram_counts.toarray(), columns=gramvec.get_feature_names())\n",
    "gram_counts = gram_counts.sum().reset_index()\n",
    "gram_counts.columns = [\"ngram\", \"frequency\"]\n",
    "gram_counts[\"n\"] = gram_counts.ngram.apply(lambda x: len(x.split(\" \")))\n",
    "\n",
    "f, axs = plt.subplots(1,3,figsize=(25,10))\n",
    "for ax, n in zip(axs, gram_counts.n.unique()):\n",
    "    gram_counts[gram_counts.n==n].sort_values(\"frequency\").tail(15).\\\n",
    "        plot(y=\"frequency\",x=\"ngram\", kind=\"barh\", ax=ax, legend=\"false\")\n",
    "    ax.set_title(str(n)+\"-gram\");\n",
    "    ax.set_ylabel(\"\");\n",
    "    ax.set_xlabel(\"frequency\");\n",
    "    ax.get_legend().remove();\n",
    "f.tight_layout()  "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# upos\n",
    "def get_upos(text,i):\n",
    "    import spacy\n",
    "    nlp =  spacy.load(\"en_core_web_sm\")\n",
    "    parsed = nlp(text)\n",
    "    ls = [(i, t.text, t.lemma_, t.pos_, t.tag_, t.dep_,\n",
    "    t.shape_, t.is_alpha, t.is_stop) for t in parsed]\n",
    "    return pd.DataFrame(ls,\n",
    "        columns=[\"doc_id\",\"text\", \"lemma\", \"pos\", \"tag\",\n",
    "            \"dep\", \"shape\", \"is_alpha\",\"is_stopword\"])\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "upos_ls = Parallel(n_jobs=4)(delayed(get_upos)(docs_augmented.OutputChars.loc[i],i)\\\n",
    "    for i in docs_augmented.index)\n",
    "docs_upos = pd.concat(upos_ls)\n",
    "\n",
    "# common ones\n",
    "docs_upos.pos.value_counts().sort_values().tail(5)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# barh noun, verb, adj\n",
    "upos_subset = [\"NOUN\",\"VERB\",\"ADJ\"]\n",
    "f, axs = plt.subplots(1,3,figsize=(25,10))\n",
    "for ax, up in zip(axs, upos_subset):\n",
    "    docs_upos[docs_upos.pos==up].lemma.value_counts().sort_values().tail(15).\\\n",
    "        plot(kind=\"barh\", ax=ax, legend=\"false\")\n",
    "    ax.set_title(up)\n",
    "    ax.set_xlabel(\"frequency\");\n",
    "    ax.get_legend().remove()\n",
    "f.tight_layout()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# rake - example\n",
    "import spacy\n",
    "from rake_spacy import Rake\n",
    "\n",
    "def filter_tokens(token):\n",
    "    return (token.is_stop or token.is_space or token.is_punct) and not (token.like_num)\\\n",
    "        and (token.pos_ not in [\"VERB\", \"ADJ\", \"NOUN\"]) and (len(token.text)<2)\n",
    "\n",
    "rake = Rake(nlp=spacy.load(\"en_core_web_sm\"), min_length=2, max_length=5,\n",
    "    stop_token_class=filter_tokens)\n",
    "\n",
    "rake.apply(docs_augmented.OutputChars[0])[:10]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# coocurence\n",
    "import numpy as np\n",
    "doc_reconstructed = docs_upos[[p in [\"NOUN\", \"VERB\",\"ADJ\"] for p in docs_upos.pos]].groupby(\"doc_id\").\\\n",
    "    apply(lambda x: \" \".join(x[\"lemma\"]))\n",
    "coovec = CountVectorizer(stop_words=\"english\",\n",
    "    ngram_range=(2,2), max_df=0.95, min_df=2)\n",
    "coo_counts = coovec.fit_transform(doc_reconstructed)\n",
    "coo_counts = pd.DataFrame(np.sum(coo_counts.todense(),axis=0),\n",
    "    columns=coovec.get_feature_names()).T.reset_index()\n",
    "coo_counts.columns = [\"bigram\",\"frequency\"]\n",
    "coo_counts.sort_values(\"frequency\").tail(10)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# networkx\n",
    "import networkx as nx\n",
    "coo_counts[\"from\"]=coo_counts.bigram.apply(lambda x: x.split(\" \")[0])\n",
    "coo_counts[\"to\"]=coo_counts.bigram.apply(lambda x: x.split(\" \")[1])\n",
    "net = nx.convert_matrix.from_pandas_edgelist(coo_counts.sort_values(\"frequency\").tail(50),\n",
    "    source=\"from\", target=\"to\", edge_attr=\"frequency\")\n",
    "f,ax = plt.subplots(1,1,figsize=(15,7))\n",
    "pos = nx.spring_layout(net, seed=1, iterations=15)\n",
    "nx.draw_networkx_labels(net, pos, font_size=10,\n",
    "    font_family=\"sans-serif\", alpha=.9, ax=ax);\n",
    "nx.draw_networkx_edges(net, width=[0.015*net[u][v]['frequency'] for u,v in net.edges()],\n",
    "    pos=pos, alpha=.25);"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# lda topics\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "\n",
    "def plot_top_words(model, feature_names, n_top_words, title):\n",
    "    fig, axes = plt.subplots(1, 5, figsize=(30, 7), sharex=True)\n",
    "    axes = axes.flatten()\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        top_features_ind = topic.argsort()[:-n_top_words - 1:-1]\n",
    "        top_features = [feature_names[i] for i in top_features_ind]\n",
    "        weights = topic[top_features_ind]\n",
    "\n",
    "        ax = axes[topic_idx]\n",
    "        ax.barh(top_features, weights, height=0.7)\n",
    "        ax.set_title(f'Topic {topic_idx +1}',\n",
    "                     fontdict={'fontsize': 12})\n",
    "        ax.invert_yaxis()\n",
    "        ax.tick_params(axis='both', which='major', labelsize=12)\n",
    "        for i in 'top right left'.split():\n",
    "            ax.spines[i].set_visible(False)\n",
    "        fig.suptitle(title, fontsize=14)\n",
    "\n",
    "    plt.subplots_adjust(top=0.90, bottom=0.05, wspace=0.90, hspace=0.3)\n",
    "    plt.show()\n",
    "\n",
    "tfidfvec = TfidfVectorizer(min_df=2)\n",
    "\n",
    "tfidf_counts = tfidfvec.fit_transform(doc_reconstructed)\n",
    "\n",
    "lda = LatentDirichletAllocation(n_components=5, max_iter=10,\n",
    "    learning_method='online', learning_offset=50., random_state=0)\n",
    "\n",
    "lda.fit(tfidf_counts)\n",
    "plot_top_words(lda, tfidfvec.get_feature_names(), 5, 'Topics in LDA model')"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('env': venv)"
  },
  "interpreter": {
   "hash": "869e4d1e7ddd42a66e036976948220a197e7d4eeb5f94674d9bf266d7eab4c92"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}